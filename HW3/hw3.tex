\documentclass[addpoints]{exam}

\usepackage{hyperref}
\usepackage[capitalise,nameinlink]{cleveref}
\usepackage{caption}
\usepackage{graphbox}
\usepackage{multirow}
\usepackage{pythonhighlight}
\usepackage{ragged2e}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{titling}
\usepackage{xcolor}

% Header and footer.
\pagestyle{headandfoot}
\runningheadrule
\runningfootrule
\runningheader{CS 201, Spring 2022}{HW 3: Information Retrieval}{\theauthor}
\runningfooter{}{Page \thepage\ of \numpages}{}
\firstpageheader{}{}{}

\qformat{{\large\bf \thequestion. \thequestiontitle}\hfill[\totalpoints\ points]}
% \qformat{{\large\bf \thequestion. \thequestiontitle}\hfill}
\boxedpoints

\printanswers

\graphicspath{{images/}}

\newcommand\colheader[1]{\multicolumn{1}{c}{#1}} % Note: no vertical bars

\title{Homework 3: Information Retrieval}
\author{unknown}  % <=== Enter your team name.
\date{CS 201 Data Structures 2\\Spring 2022}

\begin{document}
\maketitle

\begin{figure}[h]
  \centering
  \includegraphics[width=.9\textwidth]{guardian}
  \caption{\href{https://www.theguardian.com/world/coronavirus-outbreak}{Coronavirus Outbreak $|$ The Guardian}, accessed Sunday, 22 March, 2020.}
  \label{fig:guardian}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}{.7\textwidth}
    \includegraphics[width=\textwidth]{autocomplete}
    \caption{An example of auto-complete suggestions from \url{https://www.goole.com}.}
    \label{fig:autocomplete}
  \end{subfigure}
  \begin{subfigure}{.2\textwidth}
    \includegraphics[width=\textwidth]{moogle}
    \caption{Not this \href{https://finalfantasy.fandom.com/wiki/Final_Fantasy_Wiki}{Moogle}!}
    \label{fig:moogle}
  \end{subfigure}
\end{figure}

In this assignment you will build Moogle (My Google), a system to perform information retrieval tasks on a corpus. Specifically, Moogle will perform 2 tasks.
\begin{enumerate}
\item Given a query and a corpus, find completion matches for the query from the corpus. For example, see Figure \ref{fig:autocomplete}.
\item Given a query and a corpus, retrieve a list of documents from the corpus ranked according to their relevance to the query.
\end{enumerate}

The first task is supported by building a trie with all the words in the corpus. The second is supported by an inverted index built from all the documents in the corpus. You will correspondingly write and implement 3 classes: \pyth{Corpus}, \pyth{Trie}, and \pyth{InvertedIndex}.

\paragraph{\pyth{Corpus}} This class encapsulates a \texttt{Trie} instance and an \texttt{InvertedIndex} instance in order to support completion and search queries on a corpus as described above by delegating to the appropriate member structure. A \pyth{Corpus} instance is initiated with the path to a ZIP file containing the documents to be processed. The documents are text files which may or may not have a \texttt{.txt} extension. The unzipped directory may or may not contain sub-directries. The text files may be at the root level of the unzipped directory or in sub-directories. The corpus must be able to find and process all contained documents regardless. The ID of each document is its path relative to the unzipped directory. Some example corpora are listed in \cref{sec:corpora} for your testing. The class offers the methods \pyth{prefix_complete()} and \pyth{query()} by delegating to the appropriate member. The details of these are given below.

\paragraph{\pyth{Trie}} This class represents a trie (standard or compressed, your choice). Specifically, an instance of this class is used by \pyth{Corpus} to implement the \pyth{prefix_complete()} method. This class offers a method of the same name which behaves as follows. It accepts a \pyth{string} argument which is the \pyth{prefix} for which completions from the corpus are sought. It returns a dictionary in which each key is a completion from the corpus and the corresponding value is a \pyth{list} of 3-\pyth{tuple}s representing the \textit{location} information of the completion. That is, it contains the ID of the document that contains the completion and the starting and ending indexes of the completion in the document. Indexes start from 0.

\paragraph{\pyth{InvertedIndex}} This class represents an inverted index. Specifically, an instance of this class is used by \pyth{Corpus} to implement the \pyth{query()} method. This class offers a method of the same name which behaves as follows. It accepts a \pyth{string} and an \pyth{int} argument representing the \pyth{query} term and the number of desired results. Note that query may be a space separated list of multiple query terms in which case all of the contains terms form the query. It returns a sorted \pyth{list} of 2-\pyth{tuple}s (or \textit{pairs}) representing the ranked list of documents. That is, each pair contains the rank and corresponding document ID. Ranking is according to relevance of the document with the query. The most relevant document is ranked 1, the next most relevant is ranked 2, and so on. Relevance is to be computed using TF-IDF scores. The result list includes the top-\pyth{k} results only.


\section{Tasks and Implementation}

Write your \pyth{Corpus} class in the accompanying file, \texttt{src/corpus.py}. Feel free to add other files as appropriate under \texttt{src/}, provided that importing \texttt{src/corpus.py} also imports the needed files.

\subsection{Tokenization}

An important operation in this context is \href{https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html}{tokenization} which breaks a long string into smaller strings or \textit{tokens} which are more appropriate for the application. There is no \textit{correct} or \textit{standard} tokenization, rather different applications require the string to be tokenized differently. You can use this operation to break a document into terms.

\subsection{Testing}

Your submission will be tested automatically on GitHub via \pyth{pytest}. The test files will be made available shortly. Testing involves the creation of an index on the corpus which is a lengthy process. Optimize your code so as to meet the \pyth{pytest} limit of 5 minutes. A timed out test is a failed test.

Once you have successfully implemented your classes, you can test your code by applying it to the sample corpora listed below. You may create some smaller copora of your own for initial testing. For grading purposes, your submission will be tested automatically on GitHub using \pyth{pytest}. The test files will import \texttt{src/corpus.py}. You should ensure that other needed files, if any, get \pyth{import}ed as a result.

\subsection{Allowed modules}

As you have found out, \pyth{pytest} on GitHub fails if your code \pyth{import}s arbitrary modules. The allowed modules for this assignment are \pyth{pathlib} (\href{https://docs.python.org/3/library/pathlib.html}{doc}, \href{https://realpython.com/python-pathlib/}{RP}), \pyth{zipfile} (\href{https://docs.python.org/3/library/zipfile.html}{doc}, \href{https://realpython.com/python-zipfile/}{RP}), and \pyth{nltk} (\href{https://www.nltk.org}{doc}, \href{https://realpython.com/nltk-nlp-python/}{RP}). Modules that are part of python by default, e.g. \pyth{math}, can also be used.

\section{Corpora}
\label{sec:corpora}

You are free to use any corpus of your choice. \href{https://datasetsearch.research.google.com}{Google Dataset Search} and \href{https://www.kaggle.com/datasets}{Kaggle} are excellent resources for datasets. You may create your own corpus as well. Below are details of some specific datasets.
\begin{enumerate}
\item The \textit{Guardian corpus} consists of 3028 news articles published between 1 January, 2020 and 21 March, 2020 matching the keyword, ``coronavirus'', and retrieved through \href{https://open-platform.theguardian.com}{the API} of \href{https://www.theguardian.com/}{The Guardian}. It is linked on the Canvas main page under the ``HW 3'' heading in the ``Assessments and related'' module.
\item \quote{The \textit{20 Newsgroups} data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.} More details including a download link are available \href{http://qwone.com/~jason/20Newsgroups/}{here}.
\item The \textit{StackSample} dataset contains text from 10\% of Stack Overflow questions and answers on programming topics. Further details including a download link are available \href{https://www.kaggle.com/datasets/stackoverflow/stacksample?fbclid=IwAR0kFAaMfLW9DViWaRapXd6xhdGmsnM56hExLL9aVqNfOPBeLvBOxkel85g}{here}.
\end{enumerate}
   
\section{Some Information Retrieval Rambling}
\label{sec:refine}

Congratulations, you have implemented your (very first) search engine! Be proud and play around with Moogle. Go over some of the documents, perform some searches, verify them, try out some completion results, and so on.

In so doing, you will begin to realize some quirks. You may come across strange characters (these are due to unhandled Unicode characters in the original documents). Stop words will pop up. Punctuation is not correctly handled. Some of the original documents are also strange--they contain little to no content, more strange characters. All of this is common in information retrieval.

This section lists some refinements to make Moogle even more awesome! The tasks in this section are \textbf{not required and do not carry marks}. They are listed as suggestions for your own tinkering pleasure!

\subsection{Document Cleaning (Garbage In Garbage Out)}

Your results are only as good as your input and the quirks mentioned above are typical problems faced in Information Retrieval. That is why significant effort is spent on \textit{document cleaning}, i.e. pre-processing the documents to an appropriate form. This usually involves the following.

\paragraph{Stop Words and Punctuation} How should your system handle stop words and punctuation? The usual practice is to leave them out.

\paragraph{Stemming} Should documents containing the word ``doctors'' match a query for ``doctor''? How about ``isolate'' and ``isolation''? Should ``driving'' appear as a completion for ``drive''? The usual answer is ``yes''. These pairs of words are said to have the same \textit{stem} and reducing a word to its stem is called \textit{stemming}. You can best decide at what level to perform stemming--at the document level, for the trie, or for the index.

\paragraph{Others} How about case sensitivity, words with apostrophe, e.g. ``don't'', how to handle quotation marks, and initials, e.g. ``George W. Bush''?

\subsection{Even More} The next level of search is ``semantic search'' where matching takes into account not only keywords but also their \textit{meaning}, e.g. the system can distinguish between ``who'' and ``WHO'', between ``pen'', the writing instrument, and ``pen'', the holding area for animals. Such pairs of words are called \textit{homonyms} and are one of the many exciting challenges that Information Retrieval deals with.

\paragraph{\texttt{nltk}} As we see above, Information Retrieval has strong overlaps with Natural Language Processing (NLP). As such you may find the \href{https://www.nltk.org}{\textit{Natural Language Toolkit (ntlk)}} in python to be especially useful as you refine Moogle.

\section*{Credits}

This homework and related files are due in part to \href{http://qasimpasta.info}{Muhammad Qasim Pasta} and \href{http://unaizahsan.com}{Unaiza Ahsan}.

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
