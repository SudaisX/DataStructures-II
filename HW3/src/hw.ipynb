{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "# import nltk\n",
    "from zipfile import ZipFile, is_zipfile\n",
    "from pprint import pprint\n",
    "# from trie import *\n",
    "# from index import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}      #Using Hashmap instead of an array because it's more efficient\n",
    "        self.endOfWord = False  #Indicate if it's the end of a word\n",
    "        self.locs = []\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self, documents=[]):\n",
    "        self.root = TrieNode() #Initializing root node for the Trie\n",
    "        \n",
    "        # Adds all the documents to the trie\n",
    "        for doc in documents:\n",
    "            for word in doc.wordsTuples:\n",
    "                self.insert(word[0], word[1])\n",
    "\n",
    "    def insert(self, word:str, locs) -> None:\n",
    "        curr = self.root                            #Start with the root node\n",
    "\n",
    "        for char in word:                           #For every character in the word\n",
    "            if char not in curr.children:           #If the character does not exist\n",
    "                curr.children[char] = TrieNode()    #then make a new node of that character\n",
    "            curr = curr.children[char]              #Make curr the next character node that already exists or was created\n",
    "        curr.endOfWord = True                       #Mark that the word ends here\n",
    "\n",
    "        #Add location\n",
    "        curr.locs.append(locs)\n",
    "\n",
    "    def search(self, word:str) -> bool:\n",
    "        curr = self.root                    #Start with the root node\n",
    "\n",
    "        for char in word:                   #For every character in the word\n",
    "            if char not in curr.children:   #If the character does not exist\n",
    "                return False                #The word does not exist\n",
    "            curr = curr.children[char]      #Make curr the next character node if that character exists\n",
    "        return curr.endOfWord               #The word exists\n",
    "\n",
    "    def prefixExists(self, prefix: str) -> bool:\n",
    "        curr = self.root                        #Start with the root node\n",
    "\n",
    "        for char in prefix:                     #For every character in the word\n",
    "            if char not in curr.children:       #If the character does not exist\n",
    "                return False                    #The prefix does not exist\n",
    "            curr = curr.children[char]          #Make curr the next character node if that character exists\n",
    "        return True                             #The prefix exists\n",
    "\n",
    "    def _prefixSearchHelper(self, word, curr):\n",
    "        if curr.endOfWord:      #If there is a delimeter node then append the word to the hashmap\n",
    "            self._prefixWords[word] = curr.locs\n",
    "\n",
    "        for char, node in curr.children.items():\n",
    "            self._prefixSearchHelper(word + char, node) \n",
    "\n",
    "    def prefixSearch(self, prefix: str) -> list:\n",
    "        curr = self.root                        #Start with the root node\n",
    "\n",
    "        for char in prefix:                     #For every character in the word\n",
    "            if char not in curr.children:       #If the character does not exist\n",
    "                return []                       #The prefix does not exist\n",
    "            curr = curr.children[char]          #Make curr the next character node if that character exists\n",
    "\n",
    "        self._prefixWords = {}                  #An empty hashmap for the list of words returneed by the prefix search\n",
    "        self._prefixSearchHelper(prefix, curr)\n",
    "\n",
    "        return self._prefixWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, docId, data):\n",
    "        self.docId = docId\n",
    "        self.data = data\n",
    "        \n",
    "        self._tokenize()\n",
    "\n",
    "    def _tokenize(self):\n",
    "        self.words = []\n",
    "        self.wordsDict = {}\n",
    "        self.wordsTuples = []\n",
    "        for word in re.finditer(r'\\S+', self.data):\n",
    "            #append word to the list of words in the document\n",
    "            self.words.append(word.group(0))\n",
    "\n",
    "            # To generate dictionary in the format {Word: [(Document ID, Starting Index, Ending Index)]..}\n",
    "            if self.wordsDict.get(word.group(0)) == None:\n",
    "                self.wordsDict[word.group(0)] = [(word.start(), word.end())]\n",
    "            else:\n",
    "                self.wordsDict[word.group(0)].append((word.start(), word.end()))\n",
    "            \n",
    "            # To generate list of words in the format (Word, (Document ID, Starting Index, Ending Index))\n",
    "            wordTuple = [word.group(0), ((self.docId, word.start(), word.end()))]\n",
    "            self.wordsTuples.append(wordTuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    def __init__(self, documents=[]) -> None:\n",
    "        self.index = {}\n",
    "        self.documents = documents\n",
    "        self.totalDocuments = len(documents)\n",
    "\n",
    "        # Adds all the documents to the index\n",
    "        for doc in documents:\n",
    "            self._insertDoc(doc)\n",
    "\n",
    "    \n",
    "    def _insertDoc(self, doc): \n",
    "        for key in doc.wordsDict:\n",
    "            if key not in self.index:\n",
    "                self.index[key] = [(doc.docId, len(doc.wordsDict[key]))]\n",
    "            else:\n",
    "                self.index[key].append((doc.docId, len(doc.wordsDict[key])))\n",
    "\n",
    "        \n",
    "    def query(self, query: str, desiredResults: int):\n",
    "        query = query.split()\n",
    "\n",
    "        # finding tf for query\n",
    "        tf = [[0 for col in range(self.totalDocuments)] for row in range(len(query))]\n",
    "\n",
    "        for doc in range(self.totalDocuments):\n",
    "            for word in range(len(query)):\n",
    "                tf[word][doc] = self._findOccurences(query[word], self.documents[doc].docId) / len(self.documents[doc].words)\n",
    "\n",
    "        \n",
    "        # finding idf for query\n",
    "        idf = [math.log2(self.totalDocuments / len(self.index[word])) for word in query]\n",
    "\n",
    "        # finding tf-idf for the query now\n",
    "        tf_idf = [[0 for col in range(self.totalDocuments)] for row in range(len(query))]\n",
    "\n",
    "        for doc in range(self.totalDocuments):\n",
    "            for word in range(len(query)):\n",
    "                tf_idf[word][doc] = tf[word][doc] * idf[word]\n",
    "\n",
    "        #compute sum of the found tf-idf\n",
    "        final_tf_idf = [0 for col in range(self.totalDocuments)]\n",
    "\n",
    "        for doc in range(self.totalDocuments):\n",
    "            for word in range(len(query)):\n",
    "                final_tf_idf[doc] += tf_idf[word][doc]\n",
    "\n",
    "        #computing rank now (AAAA i didnt store the document ID and now im v lazy to change the structure so pls don't mind what i'm about to do)\n",
    "        ranks = []\n",
    "        for i in range(len(final_tf_idf)):\n",
    "            if final_tf_idf[i] != 0:\n",
    "                ranks.append((final_tf_idf[i], self.documents[i].docId))\n",
    "        ranks.sort(reverse=True)\n",
    "        \n",
    "        finalRanks = []\n",
    "\n",
    "        for i in range(1, len(ranks) + 1):\n",
    "            finalRanks.append((i, ranks[i-1][1]))\n",
    "\n",
    "        return finalRanks\n",
    "                \n",
    "\n",
    "    def _findOccurences(self, word, docId):\n",
    "        occurences = self.index[word]\n",
    "\n",
    "        for occurence in occurences:\n",
    "            if occurence[0] == docId:\n",
    "                return occurence[1]\n",
    "                \n",
    "        return 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, path) -> None:\n",
    "        \n",
    "        self.documents = []\n",
    "        \n",
    "        if is_zipfile(path):\n",
    "            file = ZipFile(path, mode = 'r')\n",
    "            print(file)\n",
    "            filePaths = file.namelist()\n",
    "            file.extractall() \n",
    "        elif os.path.isdir(path):\n",
    "            #files = os.listdir(path)\n",
    "            filePaths = list()\n",
    "            for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "                filePaths += [os.path.join(dirpath, file) for file in filenames]\n",
    "        else:\n",
    "            filePaths = [path]\n",
    "        \n",
    "        for filePath in filePaths:\n",
    "            if os.path.isfile(filePath):\n",
    "                with open(filePath, 'r', errors = 'ignore') as f:\n",
    "\n",
    "                    text = f.read().lower()\n",
    "                    newtext = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "                doc = Document(filePath, newtext)\n",
    "                # print(filePath)\n",
    "                self.documents.append(doc)\n",
    "\n",
    "        self.trie = Trie(self.documents)  #Creates a Trie object from the corpus\n",
    "        \n",
    "        self.index = InvertedIndex(self.documents)\n",
    "\n",
    "    def prefix_complete(self, query:str):\n",
    "        '''\n",
    "            Turn the query into lowercase since every word in the Corpus is lowercase\n",
    "            Then call prefix_complete method of the trie object of class Trie to get a list of prefix matches\n",
    "            Return the list of Prefix Matches\n",
    "        '''\n",
    "        query = query.lower()\n",
    "        return self.trie.prefixSearch(query)\n",
    "\n",
    "    def query(self, query: str, desiredResults: int):\n",
    "        return self.index.query(query, desiredResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zipfile.ZipFile filename='corpus.zip' mode='r'>\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(\"corpus.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'return': [('1-10/6-10/10', 9344, 9350),\n",
      "            ('1-10/6-10/10', 19408, 19414),\n",
      "            ('1-10/6-10/10', 30469, 30475),\n",
      "            ('1-10/1-5/4.txt', 6501, 6507),\n",
      "            ('1-10/1-5/3.txt', 5739, 5745),\n",
      "            ('1-10/1-5/3.txt', 7314, 7320),\n",
      "            ('1-10/1-5/2.txt', 8, 14),\n",
      "            ('1-10/1-5/2.txt', 15069, 15075),\n",
      "            ('1-10/1-5/1.txt', 4734, 4740),\n",
      "            ('1-10/1-5/1.txt', 26610, 26616),\n",
      "            ('11-15/14.txt', 4652, 4658),\n",
      "            ('11-15/14.txt', 27747, 27753),\n",
      "            ('11-15/14.txt', 29322, 29328),\n",
      "            ('11-15/13.txt', 3409, 3415),\n",
      "            ('11-15/13.txt', 5490, 5496),\n",
      "            ('11-15/13.txt', 10348, 10354),\n",
      "            ('11-15/13.txt', 17055, 17061),\n",
      "            ('11-15/12.txt', 1689, 1695),\n",
      "            ('11-15/12.txt', 13416, 13422)],\n",
      " 'returned': [('1-10/6-10/9', 8566, 8574),\n",
      "              ('1-10/6-10/9', 13674, 13682),\n",
      "              ('1-10/6-10/9', 17100, 17108),\n",
      "              ('1-10/6-10/9', 19094, 19102),\n",
      "              ('1-10/6-10/8', 6597, 6605),\n",
      "              ('1-10/1-5/4.txt', 28434, 28442),\n",
      "              ('1-10/1-5/4.txt', 29568, 29576)],\n",
      " 'returning': [('1-10/6-10/9', 3079, 3088),\n",
      "               ('1-10/6-10/9', 8657, 8666),\n",
      "               ('1-10/6-10/9', 13580, 13589),\n",
      "               ('1-10/1-5/4.txt', 29150, 29159),\n",
      "               ('11-15/12.txt', 7216, 7225),\n",
      "               ('18', 24916, 24925)],\n",
      " 'returns': [('11-15/13.txt', 1190, 1197),\n",
      "             ('11-15/13.txt', 3013, 3020),\n",
      "             ('11-15/13.txt', 4526, 4533),\n",
      "             ('11-15/13.txt', 5614, 5621),\n",
      "             ('11-15/13.txt', 14658, 14665),\n",
      "             ('11-15/12.txt', 14348, 14355)]}\n"
     ]
    }
   ],
   "source": [
    "pprint(corpus.prefix_complete('return'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hen']\n"
     ]
    }
   ],
   "source": [
    "print('hen'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['corpus/16', 'corpus/17', 'corpus/18', 'corpus/1-10\\\\1-5\\\\1.txt', 'corpus/1-10\\\\1-5\\\\2.txt', 'corpus/1-10\\\\1-5\\\\3.txt', 'corpus/1-10\\\\1-5\\\\4.txt', 'corpus/1-10\\\\1-5\\\\5.txt', 'corpus/1-10\\\\6-10\\\\10', 'corpus/1-10\\\\6-10\\\\6', 'corpus/1-10\\\\6-10\\\\7', 'corpus/1-10\\\\6-10\\\\8', 'corpus/1-10\\\\6-10\\\\9', 'corpus/11-15\\\\11.txt', 'corpus/11-15\\\\12.txt', 'corpus/11-15\\\\13.txt', 'corpus/11-15\\\\14.txt', 'corpus/11-15\\\\15.txt']\n",
      "corpus/16\n",
      "corpus/17\n",
      "corpus/18\n",
      "corpus/1-10/1-5/1.txt\n",
      "corpus/1-10/1-5/2.txt\n",
      "corpus/1-10/1-5/3.txt\n",
      "corpus/1-10/1-5/4.txt\n",
      "corpus/1-10/1-5/5.txt\n",
      "corpus/1-10/6-10/10\n",
      "corpus/1-10/6-10/6\n",
      "corpus/1-10/6-10/7\n",
      "corpus/1-10/6-10/8\n",
      "corpus/1-10/6-10/9\n",
      "corpus/11-15/11.txt\n",
      "corpus/11-15/12.txt\n",
      "corpus/11-15/13.txt\n",
      "corpus/11-15/14.txt\n",
      "corpus/11-15/15.txt\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "path = 'corpus/'\n",
    "filePaths = list()\n",
    "for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "    filePaths += [os.path.join(dirpath, file) for file in filenames]\n",
    "\n",
    "print(filePaths)\n",
    "\n",
    "import pathlib\n",
    "\n",
    "filePaths = [pathlib.PureWindowsPath(p).as_posix() for p in filePaths]\n",
    "print(filePaths)\n",
    "\n",
    "\n",
    "# for i in range(len(filePaths)):\n",
    "#     filePaths[i] = str(filePaths[i][len(path):]).replace(f'\\\\\\\\', '/')\n",
    "#     # print(filePaths[i])\n",
    "\n",
    "# for i in range(len(filePaths)):\n",
    "#     print(filePaths[i])\n",
    "\n",
    "# print(filePaths)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcaf9f76f83385cb97ba2d752f0108ac489eb7c7fe992d70907f64d3626af954"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
